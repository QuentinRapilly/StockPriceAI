{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_price_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYDl7Exa2OXq",
        "outputId": "d6781dbb-2596-4a37-98df-e4f2badba26d"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.67-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.10.8)\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.4 yfinance-0.1.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOYOZKc82X8V"
      },
      "source": [
        "## Stock DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PW3FBdA2XJn"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Union\n",
        "import torch\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3hNzf_62Yh9"
      },
      "source": [
        "class StockPriceDataset(Dataset):\n",
        "    def __init__(self, start_date: str=\"jj-mm-aaaa\", end_date: str=\"jj-mm-aaaa\", \n",
        "                 interval: int=1, nb_samples: int=20, transform=None,\n",
        "                 file_dir: str=\"data/\", csv_file: str=None):\n",
        "\n",
        "        # If a local data file must be loaded:\n",
        "        if csv_file is not None:\n",
        "            self.root_dir = file_dir\n",
        "            self.filename = csv_file\n",
        "            with open(os.path.join(file_dir,csv_file), 'r') as file:\n",
        "                data = pd.read_csv(file, sep=',', header='infer')\n",
        "\n",
        "        else: # Data must be loaded on an online database:\n",
        "            dataset = yf.download('^GSPC', start=start_date, end=end_date, interval=interval)\n",
        "\n",
        "        self.data = dataset\n",
        "        self.nb_samples = nb_samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)//self.nb_samples\n",
        "\n",
        "    def __getitem__(self, index) -> Union[torch.Tensor, float]:\n",
        "        # Load one sample more than nb_samples for normalizing, transform\n",
        "        sample = self.data['Close'][index*self.nb_samples:(index+1)*self.nb_samples+1]\n",
        "        # sample = self.data['Close'][index:index+self.nb_samples+1]\n",
        "        sample = torch.tensor(sample)\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)[1:]\n",
        "        else:\n",
        "            sample = sample[1:]\n",
        "        \n",
        "        label = sample[-1] # label is the last elem of sample\n",
        "\n",
        "        sample = sample[:-1] # removes label from sample\n",
        "        return sample, label\n",
        "\n",
        "def normalize_by_last_unknown_price(sample: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Divides the whole stock price sample by the last unknown price w_{p*t-1}\"\"\"\n",
        "    last_price = sample[0] # w_{pt-1}\n",
        "    return sample/last_price"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VISUALIZE_DATASET = True\n",
        "if VISUALIZE_DATASET:\n",
        "  dataset = StockPriceDataset(start_date='1950-01-03', \n",
        "                              end_date='2008-11-16',\n",
        "                              interval='1d', \n",
        "                              nb_samples=15,\n",
        "                              transform=normalize_by_last_unknown_price)\n",
        "\n",
        "  print(\"head of the dataset =\", dataset.data.head()['Close'])\n",
        "\n",
        "  print(\"\\n Plot of the 'Close' data :\")\n",
        "  date = dataset.data.plot(None, ['Close'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "3boKdX_gQFH9",
        "outputId": "da02bbc5-c71f-4070-f09e-2e8148be7f7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "head of the dataset = Date\n",
            "1950-01-03    16.66\n",
            "1950-01-04    16.85\n",
            "1950-01-05    16.93\n",
            "1950-01-06    16.98\n",
            "1950-01-09    17.08\n",
            "Name: Close, dtype: float64\n",
            "\n",
            " Plot of the 'Close' data :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-PViga_2gvF"
      },
      "source": [
        "## Modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZyhPYo82poe"
      },
      "source": [
        "from torch.nn import LSTM, Module, Dropout, ModuleList"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8eW8s6T2n-s"
      },
      "source": [
        "class StockAI(Module):\n",
        "     \n",
        "    def __init__(self, input_size, lstm_size, num_layers, keep_prob) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = LSTM(self.input_size, hidden_size=1, num_layers=self.num_layers,dropout=1-keep_prob, batch_first=True)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        a, b = self.lstm(x)\n",
        "        return b[0]\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PERSa2AB2t9V"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5Xcxyb2wP2"
      },
      "source": [
        "class StockAIConfig():\n",
        "    config = {\n",
        "        \"model\":{\n",
        "            \"input_size\": 1,\n",
        "            \"lstm_size\": 128,\n",
        "            \"num_layers\": 1,\n",
        "            \"keep_prob\": 0.8\n",
        "        },\n",
        "\n",
        "        \"dataset_train\":{\n",
        "            \"start_date\": '1950-01-03',\n",
        "            \"end_date\": '2008-11-16',\n",
        "            \"interval_date\": '1d',\n",
        "            \"nb_samples\":15,\n",
        "            \"batch_size\": 16,\n",
        "            \"shuffle\":False\n",
        "        },\n",
        "        \"dataset_test\":{\n",
        "            \"start_date\": '2008-11-17',\n",
        "            \"end_date\": '2021-11-16',\n",
        "            \"interval_date\": '1d',\n",
        "            \"nb_samples\":15,\n",
        "            \"batch_size\": 16,\n",
        "            \"shuffle\":False\n",
        "        },\n",
        "\n",
        "        \"learning\":{\n",
        "            \"num_steps\": 30,\n",
        "            \"init_lr\": 1e-03,\n",
        "            \"lr_decay\": 0.99,\n",
        "            \"init_epoch\": 5,\n",
        "            \"max_epoch\": 50\n",
        "        }   \n",
        "    }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwEsvys21gH"
      },
      "source": [
        "##Prediction : Train / Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLTXAyp320x-"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import RMSprop"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mATSU_bg25Qv",
        "outputId": "55bc9984-d503-4e73-c1ef-c3a273c7417f"
      },
      "source": [
        "# Model config\n",
        "config = StockAIConfig().config\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Init of the Dataset_train\n",
        "dataset_train = StockPriceDataset(config[\"dataset_train\"][\"start_date\"], \n",
        "                            config[\"dataset_train\"][\"end_date\"],\n",
        "                            config[\"dataset_train\"][\"interval_date\"], \n",
        "                            config[\"dataset_train\"][\"nb_samples\"],\n",
        "                            transform=normalize_by_last_unknown_price)\n",
        "\n",
        "# Init dataloader of the Dataset_train\n",
        "dataloader_train = DataLoader(dataset_train, config[\"dataset_train\"][\"batch_size\"], config[\"dataset_train\"][\"shuffle\"], drop_last=True)\n",
        "\n",
        "# Init of the Dataset_test\n",
        "dataset_test = StockPriceDataset(config[\"dataset_test\"][\"start_date\"], \n",
        "                            config[\"dataset_test\"][\"end_date\"],\n",
        "                            config[\"dataset_test\"][\"interval_date\"], \n",
        "                            config[\"dataset_test\"][\"nb_samples\"],\n",
        "                            transform=normalize_by_last_unknown_price)\n",
        "\n",
        "# Init dataloader of Dataset_test\n",
        "dataloader_test = DataLoader(dataset_test, config[\"dataset_test\"][\"batch_size\"], config[\"dataset_test\"][\"shuffle\"], drop_last=True)\n",
        "\n",
        "# Init of the model\n",
        "model = StockAI(config[\"model\"][\"input_size\"],\n",
        "                config[\"model\"][\"lstm_size\"],\n",
        "                config[\"model\"][\"num_layers\"],\n",
        "                config[\"model\"][\"keep_prob\"])\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Learning rate to use along the epochs\n",
        "learning_rates = [config[\"learning\"][\"init_lr\"] * (config[\"learning\"][\"lr_decay\"] ** max(float(i + 1 - config[\"learning\"][\"init_epoch\"]), 0.0)) for i in range(config[\"learning\"][\"max_epoch\"])]\n",
        "\n",
        "# Loss\n",
        "loss_fn = MSELoss()\n",
        "optimizer = RMSprop(model.parameters(), lr=learning_rates[0], eps=1e-08)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzAMljJz7AQt",
        "outputId": "8568fcdf-a384-4722-f403-a7564b7ae4a0"
      },
      "source": [
        "len(dataloader_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQt6ZwCR3Ale",
        "outputId": "83099bab-39fb-48af-caf9-fcc0319db479"
      },
      "source": [
        "# Learning\n",
        "for epoch_step in range(config[\"learning\"][\"max_epoch\"]):\n",
        "    lr = learning_rates[epoch_step]\n",
        "    print(f\"Running for epoch {epoch_step}...\")\n",
        "    for i_batch, batch in enumerate(dataloader_train):\n",
        "        x, y = batch\n",
        "        x = torch.unsqueeze(x, -1).float()\n",
        "        y = y.float()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model.forward(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "\n",
        "        if i_batch%10==0:\n",
        "            print(f\"step: {i_batch}, loss = {loss}\")\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running for epoch 0...\n",
            "step: 0, loss = 0.9263932108879089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([1, 16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 10, loss = 0.8407235741615295\n",
            "step: 20, loss = 0.7973533868789673\n",
            "step: 30, loss = 0.774640679359436\n",
            "step: 40, loss = 0.7388665676116943\n",
            "step: 50, loss = 0.7148832082748413\n",
            "step: 60, loss = 0.6506136655807495\n",
            "Running for epoch 1...\n",
            "step: 0, loss = 0.6711170673370361\n",
            "step: 10, loss = 0.6108390688896179\n",
            "step: 20, loss = 0.5706744194030762\n",
            "step: 30, loss = 0.5450021028518677\n",
            "step: 40, loss = 0.5078538656234741\n",
            "step: 50, loss = 0.4810989797115326\n",
            "step: 60, loss = 0.42355218529701233\n",
            "Running for epoch 2...\n",
            "step: 0, loss = 0.439033567905426\n",
            "step: 10, loss = 0.3871299624443054\n",
            "step: 20, loss = 0.3540458679199219\n",
            "step: 30, loss = 0.33456283807754517\n",
            "step: 40, loss = 0.3086320757865906\n",
            "step: 50, loss = 0.29314523935317993\n",
            "step: 60, loss = 0.2550305128097534\n",
            "Running for epoch 3...\n",
            "step: 0, loss = 0.26766008138656616\n",
            "step: 10, loss = 0.2347518354654312\n",
            "step: 20, loss = 0.2171539068222046\n",
            "step: 30, loss = 0.2095564901828766\n",
            "step: 40, loss = 0.19654951989650726\n",
            "step: 50, loss = 0.19134710729122162\n",
            "step: 60, loss = 0.16665449738502502\n",
            "Running for epoch 4...\n",
            "step: 0, loss = 0.17739908397197723\n",
            "step: 10, loss = 0.1558247208595276\n",
            "step: 20, loss = 0.1462927758693695\n",
            "step: 30, loss = 0.14387467503547668\n",
            "step: 40, loss = 0.13659588992595673\n",
            "step: 50, loss = 0.13547754287719727\n",
            "step: 60, loss = 0.11737920343875885\n",
            "Running for epoch 5...\n",
            "step: 0, loss = 0.12652574479579926\n",
            "step: 10, loss = 0.11054050922393799\n",
            "step: 20, loss = 0.10468071699142456\n",
            "step: 30, loss = 0.10422167181968689\n",
            "step: 40, loss = 0.09958209097385406\n",
            "step: 50, loss = 0.1001548245549202\n",
            "step: 60, loss = 0.08591985702514648\n",
            "Running for epoch 6...\n",
            "step: 0, loss = 0.09372448176145554\n",
            "step: 10, loss = 0.0810718685388565\n",
            "step: 20, loss = 0.07723468542098999\n",
            "step: 30, loss = 0.07761001586914062\n",
            "step: 40, loss = 0.07443596422672272\n",
            "step: 50, loss = 0.07581649720668793\n",
            "step: 60, loss = 0.06421931087970734\n",
            "Running for epoch 7...\n",
            "step: 0, loss = 0.07089875638484955\n",
            "step: 10, loss = 0.06053580716252327\n",
            "step: 20, loss = 0.057987622916698456\n",
            "step: 30, loss = 0.05873899906873703\n",
            "step: 40, loss = 0.056488119065761566\n",
            "step: 50, loss = 0.05828548222780228\n",
            "step: 60, loss = 0.04864439368247986\n",
            "Running for epoch 8...\n",
            "step: 0, loss = 0.05437486618757248\n",
            "step: 10, loss = 0.04570896551012993\n",
            "step: 20, loss = 0.04405263066291809\n",
            "step: 30, loss = 0.04495902359485626\n",
            "step: 40, loss = 0.04333166033029556\n",
            "step: 50, loss = 0.045343104749917984\n",
            "step: 60, loss = 0.03721810132265091\n",
            "Running for epoch 9...\n",
            "step: 0, loss = 0.0421425886452198\n",
            "step: 10, loss = 0.03478759527206421\n",
            "step: 20, loss = 0.03377724811434746\n",
            "step: 30, loss = 0.03471941873431206\n",
            "step: 40, loss = 0.03352940082550049\n",
            "step: 50, loss = 0.035638608038425446\n",
            "step: 60, loss = 0.028720304369926453\n",
            "Running for epoch 10...\n",
            "step: 0, loss = 0.032955434173345566\n",
            "step: 10, loss = 0.02663932368159294\n",
            "step: 20, loss = 0.026110097765922546\n",
            "step: 30, loss = 0.027020111680030823\n",
            "step: 40, loss = 0.026143530383706093\n",
            "step: 50, loss = 0.02828015200793743\n",
            "step: 60, loss = 0.02234005741775036\n",
            "Running for epoch 11...\n",
            "step: 0, loss = 0.025981849059462547\n",
            "step: 10, loss = 0.02050415799021721\n",
            "step: 20, loss = 0.020340215414762497\n",
            "step: 30, loss = 0.02117912843823433\n",
            "step: 40, loss = 0.020530197769403458\n",
            "step: 50, loss = 0.022650951519608498\n",
            "step: 60, loss = 0.017515186220407486\n",
            "Running for epoch 12...\n",
            "step: 0, loss = 0.020643342286348343\n",
            "step: 10, loss = 0.01585201360285282\n",
            "step: 20, loss = 0.01596962660551071\n",
            "step: 30, loss = 0.016715966165065765\n",
            "step: 40, loss = 0.016233835369348526\n",
            "step: 50, loss = 0.01831243559718132\n",
            "step: 60, loss = 0.013845730572938919\n",
            "Running for epoch 13...\n",
            "step: 0, loss = 0.016527000814676285\n",
            "step: 10, loss = 0.012304304167628288\n",
            "step: 20, loss = 0.01264173910021782\n",
            "step: 30, loss = 0.013285020366311073\n",
            "step: 40, loss = 0.012925883755087852\n",
            "step: 50, loss = 0.014946931041777134\n",
            "step: 60, loss = 0.011042281053960323\n",
            "Running for epoch 14...\n",
            "step: 0, loss = 0.013333076611161232\n",
            "step: 10, loss = 0.009586276486515999\n",
            "step: 20, loss = 0.010097242891788483\n",
            "step: 30, loss = 0.01063389889895916\n",
            "step: 40, loss = 0.010365829803049564\n",
            "step: 50, loss = 0.012321149930357933\n",
            "step: 60, loss = 0.008892625570297241\n",
            "Running for epoch 15...\n",
            "step: 0, loss = 0.010840983130037785\n",
            "step: 10, loss = 0.0074959625490009785\n",
            "step: 20, loss = 0.008145350962877274\n",
            "step: 30, loss = 0.008576218038797379\n",
            "step: 40, loss = 0.008375732228159904\n",
            "step: 50, loss = 0.010261859744787216\n",
            "step: 60, loss = 0.007239585742354393\n",
            "Running for epoch 16...\n",
            "step: 0, loss = 0.00888671912252903\n",
            "step: 10, loss = 0.005883468315005302\n",
            "step: 20, loss = 0.006644295062869787\n",
            "step: 30, loss = 0.006972908042371273\n",
            "step: 40, loss = 0.006822627503424883\n",
            "step: 50, loss = 0.008639231324195862\n",
            "step: 60, loss = 0.005965703167021275\n",
            "Running for epoch 17...\n",
            "step: 0, loss = 0.007347196340560913\n",
            "step: 10, loss = 0.004636538214981556\n",
            "step: 20, loss = 0.005487854592502117\n",
            "step: 30, loss = 0.005719403270632029\n",
            "step: 40, loss = 0.005606384016573429\n",
            "step: 50, loss = 0.0073551274836063385\n",
            "step: 60, loss = 0.0049825310707092285\n",
            "Running for epoch 18...\n",
            "step: 0, loss = 0.006129298824816942\n",
            "step: 10, loss = 0.003670504316687584\n",
            "step: 20, loss = 0.0045958771370351315\n",
            "step: 30, loss = 0.00473643746227026\n",
            "step: 40, loss = 0.004650997929275036\n",
            "step: 50, loss = 0.006334806326776743\n",
            "step: 60, loss = 0.004223047289997339\n",
            "Running for epoch 19...\n",
            "step: 0, loss = 0.005162053741514683\n",
            "step: 10, loss = 0.002921097679063678\n",
            "step: 20, loss = 0.0039075035601854324\n",
            "step: 30, loss = 0.0039635710418224335\n",
            "step: 40, loss = 0.0038984459824860096\n",
            "step: 50, loss = 0.0055210040882229805\n",
            "step: 60, loss = 0.003636147128418088\n",
            "Running for epoch 20...\n",
            "step: 0, loss = 0.004391039721667767\n",
            "step: 10, loss = 0.00233922665938735\n",
            "step: 20, loss = 0.003376272041350603\n",
            "step: 30, loss = 0.003354439279064536\n",
            "step: 40, loss = 0.003304175566881895\n",
            "step: 50, loss = 0.004869562573730946\n",
            "step: 60, loss = 0.003182718064635992\n",
            "Running for epoch 21...\n",
            "step: 0, loss = 0.003774290904402733\n",
            "step: 10, loss = 0.0018872539512813091\n",
            "step: 20, loss = 0.002966564381495118\n",
            "step: 30, loss = 0.0028733150102198124\n",
            "step: 40, loss = 0.0028338055126369\n",
            "step: 50, loss = 0.0043462649919092655\n",
            "step: 60, loss = 0.002832663943991065\n",
            "Running for epoch 22...\n",
            "step: 0, loss = 0.0032792543061077595\n",
            "step: 10, loss = 0.0015361679252237082\n",
            "step: 20, loss = 0.0026509505696594715\n",
            "step: 30, loss = 0.002492535160854459\n",
            "step: 40, loss = 0.002460686955600977\n",
            "step: 50, loss = 0.003924462012946606\n",
            "step: 60, loss = 0.0025627571158111095\n",
            "Running for epoch 23...\n",
            "step: 0, loss = 0.0028805830515921116\n",
            "step: 10, loss = 0.0012635457096621394\n",
            "step: 20, loss = 0.0024082534946501255\n",
            "step: 30, loss = 0.0021905903704464436\n",
            "step: 40, loss = 0.002164091682061553\n",
            "step: 50, loss = 0.0035833213478326797\n",
            "step: 60, loss = 0.002355038421228528\n",
            "Running for epoch 24...\n",
            "step: 0, loss = 0.002558463718742132\n",
            "step: 10, loss = 0.0010520154610276222\n",
            "step: 20, loss = 0.002222084905952215\n",
            "step: 30, loss = 0.0019507238175719976\n",
            "step: 40, loss = 0.0019278472755104303\n",
            "step: 50, loss = 0.0033064938616007566\n",
            "step: 60, loss = 0.002195569220930338\n",
            "Running for epoch 25...\n",
            "step: 0, loss = 0.002297346480190754\n",
            "step: 10, loss = 0.0008880713139660656\n",
            "step: 20, loss = 0.002079725731164217\n",
            "step: 30, loss = 0.0017598336562514305\n",
            "step: 40, loss = 0.001739290775731206\n",
            "step: 50, loss = 0.003081107046455145\n",
            "step: 60, loss = 0.0020735301077365875\n",
            "Running for epoch 26...\n",
            "step: 0, loss = 0.0020849863067269325\n",
            "step: 10, loss = 0.0007612055051140487\n",
            "step: 20, loss = 0.0019712974317371845\n",
            "step: 30, loss = 0.001607657177373767\n",
            "step: 40, loss = 0.001588496146723628\n",
            "step: 50, loss = 0.002896989695727825\n",
            "step: 60, loss = 0.0019804995972663164\n",
            "Running for epoch 27...\n",
            "step: 0, loss = 0.0019117151387035847\n",
            "step: 10, loss = 0.0006632225122302771\n",
            "step: 20, loss = 0.001889117294922471\n",
            "step: 30, loss = 0.0014861165545880795\n",
            "step: 40, loss = 0.001467649475671351\n",
            "step: 50, loss = 0.0027460786513984203\n",
            "step: 60, loss = 0.001909921527840197\n",
            "Running for epoch 28...\n",
            "step: 0, loss = 0.0017698637675493956\n",
            "step: 10, loss = 0.0005877234507352114\n",
            "step: 20, loss = 0.0018272096058353782\n",
            "step: 30, loss = 0.0013888684334233403\n",
            "step: 40, loss = 0.0013705960009247065\n",
            "step: 50, loss = 0.0026219740975648165\n",
            "step: 60, loss = 0.0018566956277936697\n",
            "Running for epoch 29...\n",
            "step: 0, loss = 0.0016533606685698032\n",
            "step: 10, loss = 0.0005297212628647685\n",
            "step: 20, loss = 0.0017809224082157016\n",
            "step: 30, loss = 0.0013109107967466116\n",
            "step: 40, loss = 0.0012924837647005916\n",
            "step: 50, loss = 0.00251957424916327\n",
            "step: 60, loss = 0.0018168435199186206\n",
            "Running for epoch 30...\n",
            "step: 0, loss = 0.0015573533019050956\n",
            "step: 10, loss = 0.00048531003994867206\n",
            "step: 20, loss = 0.001746635651215911\n",
            "step: 30, loss = 0.0012482884339988232\n",
            "step: 40, loss = 0.0012294668704271317\n",
            "step: 50, loss = 0.002434798050671816\n",
            "step: 60, loss = 0.0017872713506221771\n",
            "Running for epoch 31...\n",
            "step: 0, loss = 0.0014779709745198488\n",
            "step: 10, loss = 0.00045144144678488374\n",
            "step: 20, loss = 0.0017215341795235872\n",
            "step: 30, loss = 0.001197876175865531\n",
            "step: 40, loss = 0.0011785052483901381\n",
            "step: 50, loss = 0.0023643793538212776\n",
            "step: 60, loss = 0.0017655710689723492\n",
            "Running for epoch 32...\n",
            "step: 0, loss = 0.0014121182030066848\n",
            "step: 10, loss = 0.00042573397513478994\n",
            "step: 20, loss = 0.0017034319462254643\n",
            "step: 30, loss = 0.0011572001967579126\n",
            "step: 40, loss = 0.001137185376137495\n",
            "step: 50, loss = 0.002305696252733469\n",
            "step: 60, loss = 0.0017498722299933434\n",
            "Running for epoch 33...\n",
            "step: 0, loss = 0.001357311848551035\n",
            "step: 10, loss = 0.0004063292872160673\n",
            "step: 20, loss = 0.0016906354576349258\n",
            "step: 30, loss = 0.0011243036715313792\n",
            "step: 40, loss = 0.001103595132008195\n",
            "step: 50, loss = 0.002256636507809162\n",
            "step: 60, loss = 0.0017387247644364834\n",
            "Running for epoch 34...\n",
            "step: 0, loss = 0.001311546890065074\n",
            "step: 10, loss = 0.0003917778958566487\n",
            "step: 20, loss = 0.001681832829490304\n",
            "step: 30, loss = 0.0010976264020428061\n",
            "step: 40, loss = 0.0010762081947177649\n",
            "step: 50, loss = 0.002215486718341708\n",
            "step: 60, loss = 0.0017310057301074266\n",
            "Running for epoch 35...\n",
            "step: 0, loss = 0.0012732078321278095\n",
            "step: 10, loss = 0.00038095051422715187\n",
            "step: 20, loss = 0.0016760125290602446\n",
            "step: 30, loss = 0.001075930893421173\n",
            "step: 40, loss = 0.0010538101196289062\n",
            "step: 50, loss = 0.0021808664314448833\n",
            "step: 60, loss = 0.0017258499283343554\n",
            "Running for epoch 36...\n",
            "step: 0, loss = 0.0012409903574734926\n",
            "step: 10, loss = 0.00037297001108527184\n",
            "step: 20, loss = 0.0016723971348255873\n",
            "step: 30, loss = 0.001058235066011548\n",
            "step: 40, loss = 0.001035434426739812\n",
            "step: 50, loss = 0.0021516485139727592\n",
            "step: 60, loss = 0.0017225921619683504\n",
            "Running for epoch 37...\n",
            "step: 0, loss = 0.0012138267047703266\n",
            "step: 10, loss = 0.00036715512396767735\n",
            "step: 20, loss = 0.0016703917644917965\n",
            "step: 30, loss = 0.0010437528835609555\n",
            "step: 40, loss = 0.0010203048586845398\n",
            "step: 50, loss = 0.00212691118940711\n",
            "step: 60, loss = 0.0017207228811457753\n",
            "Running for epoch 38...\n",
            "step: 0, loss = 0.0011908524902537465\n",
            "step: 10, loss = 0.0003629800630733371\n",
            "step: 20, loss = 0.0016695441445335746\n",
            "step: 30, loss = 0.001031859777867794\n",
            "step: 40, loss = 0.0010078039485961199\n",
            "step: 50, loss = 0.002105904510244727\n",
            "step: 60, loss = 0.0017198557034134865\n",
            "Running for epoch 39...\n",
            "step: 0, loss = 0.0011713603744283319\n",
            "step: 10, loss = 0.0003600393538363278\n",
            "step: 20, loss = 0.0016695123631507158\n",
            "step: 30, loss = 0.0010220565600320697\n",
            "step: 40, loss = 0.0009974357672035694\n",
            "step: 50, loss = 0.002088012173771858\n",
            "step: 60, loss = 0.0017196977278217673\n",
            "Running for epoch 40...\n",
            "step: 0, loss = 0.001154770259745419\n",
            "step: 10, loss = 0.00035802152706310153\n",
            "step: 20, loss = 0.0016700404230505228\n",
            "step: 30, loss = 0.001013944623991847\n",
            "step: 40, loss = 0.0009888021741062403\n",
            "step: 50, loss = 0.002072725910693407\n",
            "step: 60, loss = 0.001720030209980905\n",
            "Running for epoch 41...\n",
            "step: 0, loss = 0.001140607288107276\n",
            "step: 10, loss = 0.00035668883356265724\n",
            "step: 20, loss = 0.0016709377523511648\n",
            "step: 30, loss = 0.0010072046425193548\n",
            "step: 40, loss = 0.000981584656983614\n",
            "step: 50, loss = 0.0020596254616975784\n",
            "step: 60, loss = 0.0017206897027790546\n",
            "Running for epoch 42...\n",
            "step: 0, loss = 0.0011284772772341967\n",
            "step: 10, loss = 0.00035586004378274083\n",
            "step: 20, loss = 0.0016720648854970932\n",
            "step: 30, loss = 0.001001581083983183\n",
            "step: 40, loss = 0.0009755248902365565\n",
            "step: 50, loss = 0.0020483650732785463\n",
            "step: 60, loss = 0.0017215566476806998\n",
            "Running for epoch 43...\n",
            "step: 0, loss = 0.001118056708946824\n",
            "step: 10, loss = 0.00035539816599339247\n",
            "step: 20, loss = 0.0016733205411583185\n",
            "step: 30, loss = 0.0009968685917556286\n",
            "step: 40, loss = 0.0009704162366688251\n",
            "step: 50, loss = 0.0020386571995913982\n",
            "step: 60, loss = 0.001722543966025114\n",
            "Running for epoch 44...\n",
            "step: 0, loss = 0.001109075965359807\n",
            "step: 10, loss = 0.0003551999107003212\n",
            "step: 20, loss = 0.001674632541835308\n",
            "step: 30, loss = 0.0009929020889103413\n",
            "step: 40, loss = 0.0009660901851020753\n",
            "step: 50, loss = 0.002030263189226389\n",
            "step: 60, loss = 0.0017235891427844763\n",
            "Running for epoch 45...\n",
            "step: 0, loss = 0.001101314090192318\n",
            "step: 10, loss = 0.00035518797812983394\n",
            "step: 20, loss = 0.0016759501304477453\n",
            "step: 30, loss = 0.0009895486291497946\n",
            "step: 40, loss = 0.0009624123340472579\n",
            "step: 50, loss = 0.0020229830406606197\n",
            "step: 60, loss = 0.0017246482893824577\n",
            "Running for epoch 46...\n",
            "step: 0, loss = 0.0010945845860987902\n",
            "step: 10, loss = 0.0003553048300091177\n",
            "step: 20, loss = 0.001677239779382944\n",
            "step: 30, loss = 0.0009867013432085514\n",
            "step: 40, loss = 0.0009592718561179936\n",
            "step: 50, loss = 0.0020166514441370964\n",
            "step: 60, loss = 0.0017256918363273144\n",
            "Running for epoch 47...\n",
            "step: 0, loss = 0.0010887321550399065\n",
            "step: 10, loss = 0.00035550794564187527\n",
            "step: 20, loss = 0.0016784785548225045\n",
            "step: 30, loss = 0.0009842730360105634\n",
            "step: 40, loss = 0.0009565796935930848\n",
            "step: 50, loss = 0.0020111280027776957\n",
            "step: 60, loss = 0.0017266995273530483\n",
            "Running for epoch 48...\n",
            "step: 0, loss = 0.001083629671484232\n",
            "step: 10, loss = 0.0003557660966180265\n",
            "step: 20, loss = 0.0016796526033431292\n",
            "step: 30, loss = 0.0009821935091167688\n",
            "step: 40, loss = 0.0009542622719891369\n",
            "step: 50, loss = 0.0020062969997525215\n",
            "step: 60, loss = 0.0017276586731895804\n",
            "Running for epoch 49...\n",
            "step: 0, loss = 0.0010791668901219964\n",
            "step: 10, loss = 0.0003560565528459847\n",
            "step: 20, loss = 0.0016807548236101866\n",
            "step: 30, loss = 0.0009804051369428635\n",
            "step: 40, loss = 0.0009522599284537137\n",
            "step: 50, loss = 0.002002062276005745\n",
            "step: 60, loss = 0.0017285621725022793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2naoTex3DFi",
        "outputId": "19adaab3-7d0b-484d-f627-1bd85e318a20"
      },
      "source": [
        "#test\n",
        "runnning_mape = 0\n",
        "for i_batch, batch in enumerate(dataloader_test):\n",
        "        x, y = batch\n",
        "        x = torch.unsqueeze(x, -1).float()\n",
        "        y = y.float()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model.forward(x)\n",
        "        error = torch.mean(torch.abs((y - y_pred) / y))\n",
        "        runnning_mape += error\n",
        "\n",
        "mape = runnning_mape / len(dataloader_test)\n",
        "print(\"\",mape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " tensor(0.0294, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    }
  ]
}