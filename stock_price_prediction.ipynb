{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stock_price_prediction.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYDl7Exa2OXq","executionInfo":{"status":"ok","timestamp":1638183393553,"user_tz":-60,"elapsed":3535,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}},"outputId":"0a7dcbb8-b0d5-4498-e552-2eee63fc00e2"},"source":["!pip install yfinance"],"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.67)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n","Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.6.4)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"aOYOZKc82X8V"},"source":["## Stock DATASET"]},{"cell_type":"code","metadata":{"id":"0PW3FBdA2XJn","executionInfo":{"status":"ok","timestamp":1638183393553,"user_tz":-60,"elapsed":4,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}}},"source":["from torch.utils.data import Dataset, DataLoader\n","from typing import Union\n","import torch\n","import yfinance as yf\n","import pandas as pd\n","import numpy as np\n","import os"],"execution_count":125,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3hNzf_62Yh9","executionInfo":{"status":"ok","timestamp":1638183394190,"user_tz":-60,"elapsed":640,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}},"outputId":"a18c3164-f111-4ade-ccfd-f891c44660b6"},"source":["class StockPriceDataset(Dataset):\n","    def __init__(self, start_date: str=\"jj-mm-aaaa\", end_date: str=\"jj-mm-aaaa\", \n","                 interval: int=1, nb_samples: int=20, transform=None,\n","                 file_dir: str=\"data/\", csv_file: str=None):\n","\n","        # If a local data file must be loaded:\n","        if csv_file is not None:\n","            self.root_dir = file_dir\n","            self.filename = csv_file\n","            with open(os.path.join(file_dir,csv_file), 'r') as file:\n","                data = pd.read_csv(file, sep=',', header='infer')\n","\n","        else: # Data must be loaded on an online database:\n","            dataset = yf.download('^GSPC', start=start_date, end=end_date, interval=interval)\n","\n","        self.data = dataset\n","        self.nb_samples = nb_samples\n","        self.transform = transform\n","\n","    def __len__(self) -> int:\n","        return len(self.data)//self.nb_samples\n","\n","    def __getitem__(self, index) -> Union[torch.Tensor, float]:\n","        # Load one sample more than nb_samples for normalizing, transform\n","        sample = self.data['Close'][index*self.nb_samples:(index+1)*self.nb_samples+1]\n","        # sample = self.data['Close'][index:index+self.nb_samples+1]\n","        sample = torch.tensor(sample)\n","        if self.transform:\n","            sample = self.transform(sample)[1:]\n","        else:\n","            sample = sample[1:]\n","        \n","        label = sample[-1] # label is the last elem of sample\n","\n","        sample = sample[:-1] # removes label from sample\n","        return sample, label\n","\n","def normalize_by_last_unknown_price(sample: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Divides the whole stock price sample by the last unknown price w_{p*t-1}\"\"\"\n","    last_price = sample[0] # w_{pt-1}\n","    return sample/last_price\n","\n","\n","if __name__ == \"__main__\":\n","\n","    # download S&P data from yahoo finance\n","    START_DATE = '1950-01-03'\n","    END_DATE = '2021-11-16'\n","    INTERVAL = '1d'\n","    nb_samples = 15\n","\n","    dataset = StockPriceDataset(START_DATE, END_DATE, INTERVAL, nb_samples,\n","                                transform=normalize_by_last_unknown_price)\n","\n","    dataloader = DataLoader(dataset, batch_size=16)\n","    print(len(dataloader))\n","    # for i_batch, batch in enumerate(dataloader):\n","    #     print(\"i_batch = {}, batch = {}\".format(i_batch, batch))\n"],"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["\r[*********************100%***********************]  1 of 1 completed\n","76\n"]}]},{"cell_type":"markdown","metadata":{"id":"5-PViga_2gvF"},"source":["## ModÃ¨le"]},{"cell_type":"code","metadata":{"id":"4ZyhPYo82poe","executionInfo":{"status":"ok","timestamp":1638183394190,"user_tz":-60,"elapsed":4,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}}},"source":["from torch.nn import LSTM, Module, Dropout, ModuleList"],"execution_count":127,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8eW8s6T2n-s","executionInfo":{"status":"ok","timestamp":1638183394190,"user_tz":-60,"elapsed":4,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}}},"source":["class StockAI(Module):\n","     \n","    def __init__(self, input_size, lstm_size, num_layers, keep_prob) -> None:\n","        super().__init__()\n","        self.input_size = input_size\n","        self.lstm_size = lstm_size\n","        self.num_layers = num_layers\n","        self.lstm = LSTM(self.input_size, hidden_size = 1, num_layers = self.num_layers, dropout = 1 - keep_prob, batch_first = True)\n","    \n","    def forward(self,x):\n","        a, b = self.lstm(x)\n","        return b[0]\n","    "],"execution_count":128,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PERSa2AB2t9V"},"source":["## Configuration"]},{"cell_type":"code","metadata":{"id":"Fl5Xcxyb2wP2","executionInfo":{"status":"ok","timestamp":1638183394191,"user_tz":-60,"elapsed":4,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}}},"source":["class StockAIConfig():\n","    config = {\n","        \"model\":{\n","            \"input_size\": 1,\n","            \"lstm_size\": 128,\n","            \"num_layers\": 1,\n","            \"keep_prob\": 0.8\n","        },\n","\n","        \"dataset_train\":{\n","            \"start_date\": '1950-01-03',\n","            \"end_date\": '2008-11-16',\n","            \"interval_date\": '1d',\n","            \"nb_samples\":15,\n","            \"batch_size\": 16,\n","            \"shuffle\":False\n","        },\n","        \"dataset_test\":{\n","            \"start_date\": '2008-11-17',\n","            \"end_date\": '2021-11-16',\n","            \"interval_date\": '1d',\n","            \"nb_samples\":15,\n","            \"batch_size\": 16,\n","            \"shuffle\":False\n","        },\n","\n","        \"learning\":{\n","            \"num_steps\": 30,\n","            \"init_lr\": 1e-03,\n","            \"lr_decay\": 0.99,\n","            \"init_epoch\": 5,\n","            \"max_epoch\": 50\n","        }   \n","    }"],"execution_count":129,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhwEsvys21gH"},"source":["##Prediction : Train / Test"]},{"cell_type":"code","metadata":{"id":"RLTXAyp320x-","executionInfo":{"status":"ok","timestamp":1638183394191,"user_tz":-60,"elapsed":4,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}}},"source":["import math\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.nn import MSELoss\n","from torch.optim import RMSprop"],"execution_count":130,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mATSU_bg25Qv","executionInfo":{"status":"ok","timestamp":1638183395329,"user_tz":-60,"elapsed":1142,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}},"outputId":"a6e4933a-cf42-47a4-bc1f-3e6d3b0fcbad"},"source":["# Model config\n","config = StockAIConfig().config\n","\n","# CUDA for PyTorch\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","# Init of the Dataset_train\n","dataset_train = StockPriceDataset(config[\"dataset_train\"][\"start_date\"], \n","                            config[\"dataset_train\"][\"end_date\"],\n","                            config[\"dataset_train\"][\"interval_date\"], \n","                            config[\"dataset_train\"][\"nb_samples\"],\n","                            transform=normalize_by_last_unknown_price)\n","\n","# Init dataloader of the Dataset_train\n","dataloader_train = DataLoader(dataset_train, config[\"dataset_train\"][\"batch_size\"], config[\"dataset_train\"][\"shuffle\"], drop_last=True)\n","\n","# Init of the Dataset_test\n","dataset_test = StockPriceDataset(config[\"dataset_test\"][\"start_date\"], \n","                            config[\"dataset_test\"][\"end_date\"],\n","                            config[\"dataset_test\"][\"interval_date\"], \n","                            config[\"dataset_test\"][\"nb_samples\"],\n","                            transform=normalize_by_last_unknown_price)\n","\n","# Init dataloader of Dataset_test\n","dataloader_test = DataLoader(dataset_test, config[\"dataset_test\"][\"batch_size\"], config[\"dataset_test\"][\"shuffle\"], drop_last=True)\n","\n","# Init of the model\n","model = StockAI(config[\"model\"][\"input_size\"],\n","                config[\"model\"][\"lstm_size\"],\n","                config[\"model\"][\"num_layers\"],\n","                config[\"model\"][\"keep_prob\"])\n","\n","model.to(device)\n","\n","# Learning rate to use along the epochs\n","learning_rates = [config[\"learning\"][\"init_lr\"] * (config[\"learning\"][\"lr_decay\"] ** max(float(i + 1 - config[\"learning\"][\"init_epoch\"]), 0.0)) for i in range(config[\"learning\"][\"max_epoch\"])]\n","\n","# Loss\n","loss_fn = MSELoss()\n","optimizer = RMSprop(model.parameters(), lr=learning_rates[0], eps=1e-08)"],"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19999999999999996 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzAMljJz7AQt","executionInfo":{"status":"ok","timestamp":1638183395330,"user_tz":-60,"elapsed":6,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}},"outputId":"ef7e4d43-639c-4a54-b75c-536566bb3245"},"source":["len(dataloader_train)"],"execution_count":132,"outputs":[{"output_type":"execute_result","data":{"text/plain":["61"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQt6ZwCR3Ale","executionInfo":{"status":"ok","timestamp":1638183419283,"user_tz":-60,"elapsed":23956,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}},"outputId":"f0a00f6f-d484-49a3-f55d-e88f0c51ed98"},"source":["# Learning\n","for epoch_step in range(config[\"learning\"][\"max_epoch\"]):\n","    lr = learning_rates[epoch_step]\n","    print(f\"Running for epoch {epoch_step}...\")\n","    for i_batch, batch in enumerate(dataloader_train):\n","        x, y = batch\n","        x = torch.unsqueeze(x, -1).float()\n","        y = y.float()\n","        x, y = x.to(device), y.to(device)\n","        y_pred = model.forward(x)\n","        loss = loss_fn(y_pred, y)\n","        #loss = torch.autograd.Variable(loss_fn(y_pred, y), requires_grad=True)\n","\n","        if i_batch%10==0:\n","            print(f\"step: {i_batch}, loss = {loss}\")\n","\n","        # Zero gradients, perform a backward pass, and update the weights.\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["Running for epoch 0...\n","step: 0, loss = 0.5357494354248047\n","step: 10, loss = 0.39156851172447205\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([1, 16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["step: 20, loss = 0.3254278898239136\n","step: 30, loss = 0.2858092784881592\n","step: 40, loss = 0.2471795678138733\n","step: 50, loss = 0.222746342420578\n","step: 60, loss = 0.18136408925056458\n","Running for epoch 1...\n","step: 0, loss = 0.191830575466156\n","step: 10, loss = 0.15766489505767822\n","step: 20, loss = 0.13922575116157532\n","step: 30, loss = 0.12977278232574463\n","step: 40, loss = 0.11707652360200882\n","step: 50, loss = 0.11156065762042999\n","step: 60, loss = 0.09127508103847504\n","Running for epoch 2...\n","step: 0, loss = 0.09940649569034576\n","step: 10, loss = 0.08205941319465637\n","step: 20, loss = 0.07507139444351196\n","step: 30, loss = 0.0729711577296257\n","step: 40, loss = 0.06778103113174438\n","step: 50, loss = 0.06749594211578369\n","step: 60, loss = 0.05492448806762695\n","Running for epoch 3...\n","step: 0, loss = 0.06129622459411621\n","step: 10, loss = 0.05024588480591774\n","step: 20, loss = 0.04716656357049942\n","step: 30, loss = 0.04715589061379433\n","step: 40, loss = 0.044587019830942154\n","step: 50, loss = 0.04592587798833847\n","step: 60, loss = 0.036880239844322205\n","Running for epoch 4...\n","step: 0, loss = 0.04201536625623703\n","step: 10, loss = 0.033905789256095886\n","step: 20, loss = 0.032472070306539536\n","step: 30, loss = 0.03308796137571335\n","step: 40, loss = 0.031627342104911804\n","step: 50, loss = 0.033514443784952164\n","step: 60, loss = 0.026439929381012917\n","Running for epoch 5...\n","step: 0, loss = 0.030670206993818283\n","step: 10, loss = 0.024224940687417984\n","step: 20, loss = 0.023626701906323433\n","step: 30, loss = 0.024400632828474045\n","step: 40, loss = 0.023488780483603477\n","step: 50, loss = 0.02555079571902752\n","step: 60, loss = 0.019750647246837616\n","Running for epoch 6...\n","step: 0, loss = 0.023284588009119034\n","step: 10, loss = 0.017920756712555885\n","step: 20, loss = 0.017811492085456848\n","step: 30, loss = 0.01857421174645424\n","step: 40, loss = 0.01796828955411911\n","step: 50, loss = 0.020058881491422653\n","step: 60, loss = 0.015168190002441406\n","Running for epoch 7...\n","step: 0, loss = 0.018143756315112114\n","step: 10, loss = 0.013552429154515266\n","step: 20, loss = 0.013760792091488838\n","step: 30, loss = 0.014447320252656937\n","step: 40, loss = 0.014027466997504234\n","step: 50, loss = 0.01608496718108654\n","step: 60, loss = 0.011887861415743828\n","Running for epoch 8...\n","step: 0, loss = 0.0144018130376935\n","step: 10, loss = 0.010399010963737965\n","step: 20, loss = 0.010829850099980831\n","step: 30, loss = 0.011415986344218254\n","step: 40, loss = 0.011116803623735905\n","step: 50, loss = 0.01311476994305849\n","step: 60, loss = 0.009470739401876926\n","Running for epoch 9...\n","step: 0, loss = 0.011594673618674278\n","step: 10, loss = 0.008060229942202568\n","step: 20, loss = 0.008655540645122528\n","step: 30, loss = 0.009134471416473389\n","step: 40, loss = 0.008917206898331642\n","step: 50, loss = 0.010845127515494823\n","step: 60, loss = 0.007655641995370388\n","Running for epoch 10...\n","step: 0, loss = 0.009444940835237503\n","step: 10, loss = 0.006294482387602329\n","step: 20, loss = 0.007016121409833431\n","step: 30, loss = 0.007388845086097717\n","step: 40, loss = 0.007228991482406855\n","step: 50, loss = 0.009084059856832027\n","step: 60, loss = 0.006275818217545748\n","Running for epoch 11...\n","step: 0, loss = 0.007774917874485254\n","step: 10, loss = 0.004945741966366768\n","step: 20, loss = 0.005767091177403927\n","step: 30, loss = 0.006038224324584007\n","step: 40, loss = 0.005919424816966057\n","step: 50, loss = 0.0077026779763400555\n","step: 60, loss = 0.005218733102083206\n","Running for epoch 12...\n","step: 0, loss = 0.006464296020567417\n","step: 10, loss = 0.003907742910087109\n","step: 20, loss = 0.004809338599443436\n","step: 30, loss = 0.00498511828482151\n","step: 40, loss = 0.004896026104688644\n","step: 50, loss = 0.00661048386245966\n","step: 60, loss = 0.0044050975702703\n","Running for epoch 13...\n","step: 0, loss = 0.0054280124604702\n","step: 10, loss = 0.0031050981488078833\n","step: 20, loss = 0.004072198178619146\n","step: 30, loss = 0.004159498959779739\n","step: 40, loss = 0.004092011135071516\n","step: 50, loss = 0.005741682834923267\n","step: 60, loss = 0.0037772408686578274\n","Running for epoch 14...\n","step: 0, loss = 0.004603934474289417\n","step: 10, loss = 0.002482676412910223\n","step: 20, loss = 0.0035038175992667675\n","step: 30, loss = 0.0035096544306725264\n","step: 40, loss = 0.003457855898886919\n","step: 50, loss = 0.0050472053699195385\n","step: 60, loss = 0.0032922280952334404\n","Running for epoch 15...\n","step: 0, loss = 0.00394555926322937\n","step: 10, loss = 0.0019992724992334843\n","step: 20, loss = 0.0030653667636215687\n","step: 30, loss = 0.0029966251458972692\n","step: 40, loss = 0.0029561473056674004\n","step: 50, loss = 0.004489776212722063\n","step: 60, loss = 0.0029175684321671724\n","Running for epoch 16...\n","step: 0, loss = 0.0034174874890595675\n","step: 10, loss = 0.0016236023511737585\n","step: 20, loss = 0.002727349754422903\n","step: 30, loss = 0.002590630203485489\n","step: 40, loss = 0.0025582220405340195\n","step: 50, loss = 0.0040407078340649605\n","step: 60, loss = 0.0026284318882972\n","Running for epoch 17...\n","step: 0, loss = 0.0029924274422228336\n","step: 10, loss = 0.0013316916301846504\n","step: 20, loss = 0.002467143116518855\n","step: 30, loss = 0.00226868549361825\n","step: 40, loss = 0.0022419188171625137\n","step: 50, loss = 0.003677686210721731\n","step: 60, loss = 0.002405662089586258\n","Running for epoch 18...\n","step: 0, loss = 0.0026491463650017977\n","step: 10, loss = 0.0011050093453377485\n","step: 20, loss = 0.002267296425998211\n","step: 30, loss = 0.0020129296462982893\n","step: 40, loss = 0.001989993965253234\n","step: 50, loss = 0.003383256494998932\n","step: 60, loss = 0.00223443191498518\n","Running for epoch 19...\n","step: 0, loss = 0.002371021546423435\n","step: 10, loss = 0.0009291744790971279\n","step: 20, loss = 0.002114265924319625\n","step: 30, loss = 0.0018094079568982124\n","step: 40, loss = 0.001788953086361289\n","step: 50, loss = 0.0031436863355338573\n","step: 60, loss = 0.002103222068399191\n","Running for epoch 20...\n","step: 0, loss = 0.002144967671483755\n","step: 10, loss = 0.0007929897401481867\n","step: 20, loss = 0.0019975388422608376\n","step: 30, loss = 0.0016471856506541371\n","step: 40, loss = 0.0016282184515148401\n","step: 50, loss = 0.0029481234960258007\n","step: 60, loss = 0.0020030615851283073\n","Running for epoch 21...\n","step: 0, loss = 0.001960658933967352\n","step: 10, loss = 0.000687722465954721\n","step: 20, loss = 0.0019089254783466458\n","step: 30, loss = 0.0015176599845290184\n","step: 40, loss = 0.0014994533266872168\n","step: 50, loss = 0.002787959761917591\n","step: 60, loss = 0.0019269572803750634\n","Running for epoch 22...\n","step: 0, loss = 0.0018099001608788967\n","step: 10, loss = 0.0006065432680770755\n","step: 20, loss = 0.0018420470878481865\n","step: 30, loss = 0.0014140666462481022\n","step: 40, loss = 0.0013960966607555747\n","step: 50, loss = 0.002656368538737297\n","step: 60, loss = 0.0018694645259529352\n","Running for epoch 23...\n","step: 0, loss = 0.001686195144429803\n","step: 10, loss = 0.0005441210232675076\n","step: 20, loss = 0.001791937043890357\n","step: 30, loss = 0.0013310639187693596\n","step: 40, loss = 0.0013129618018865585\n","step: 50, loss = 0.0025478980969637632\n","step: 60, loss = 0.0018263335805386305\n","Running for epoch 24...\n","step: 0, loss = 0.0015843567671254277\n","step: 10, loss = 0.0004962820094078779\n","step: 20, loss = 0.0017547236056998372\n","step: 30, loss = 0.0012644308153539896\n","step: 40, loss = 0.0012459404533728957\n","step: 50, loss = 0.0024581884499639273\n","step: 60, loss = 0.001794249750673771\n","Running for epoch 25...\n","step: 0, loss = 0.001500243553891778\n","step: 10, loss = 0.00045976092224009335\n","step: 20, loss = 0.0017273921985179186\n","step: 30, loss = 0.001210829010233283\n","step: 40, loss = 0.0011917862575501204\n","step: 50, loss = 0.0023837541230022907\n","step: 60, loss = 0.0017706365324556828\n","Running for epoch 26...\n","step: 0, loss = 0.0014305445365607738\n","step: 10, loss = 0.00043200768413953483\n","step: 20, loss = 0.001707602059468627\n","step: 30, loss = 0.0011676170397549868\n","step: 40, loss = 0.0011479182867333293\n","step: 50, loss = 0.0023217934649437666\n","step: 60, loss = 0.0017534892540425062\n","Running for epoch 27...\n","step: 0, loss = 0.0013726004399359226\n","step: 10, loss = 0.0004110310401301831\n","step: 20, loss = 0.0016935353633016348\n","step: 30, loss = 0.001132697332650423\n","step: 40, loss = 0.0011122891446575522\n","step: 50, loss = 0.0022700498811900616\n","step: 60, loss = 0.0017412520246580243\n","Running for epoch 28...\n","step: 0, loss = 0.0013242729473859072\n","step: 10, loss = 0.00039527565240859985\n","step: 20, loss = 0.0016837851144373417\n","step: 30, loss = 0.0011044065468013287\n","step: 40, loss = 0.0010832706466317177\n","step: 50, loss = 0.0022266993764787912\n","step: 60, loss = 0.0017327198293060064\n","Running for epoch 29...\n","step: 0, loss = 0.0012838351540267467\n","step: 10, loss = 0.0003835304523818195\n","step: 20, loss = 0.001677264692261815\n","step: 30, loss = 0.0010814231354743242\n","step: 40, loss = 0.0010595645289868116\n","step: 50, loss = 0.0021902690641582012\n","step: 60, loss = 0.0017269626259803772\n","Running for epoch 30...\n","step: 0, loss = 0.0012498905416578054\n","step: 10, loss = 0.0003748529707081616\n","step: 20, loss = 0.0016731381183490157\n","step: 30, loss = 0.0010626971488818526\n","step: 40, loss = 0.00104013760574162\n","step: 50, loss = 0.0021595540456473827\n","step: 60, loss = 0.0017232648096978664\n","Running for epoch 31...\n","step: 0, loss = 0.0012213048757985234\n","step: 10, loss = 0.0003685120027512312\n","step: 20, loss = 0.0016707648755982518\n","step: 30, loss = 0.0010473908623680472\n","step: 40, loss = 0.0010241626296192408\n","step: 50, loss = 0.0021335803903639317\n","step: 60, loss = 0.0017210780642926693\n","Running for epoch 32...\n","step: 0, loss = 0.0011971569620072842\n","step: 10, loss = 0.0003639419446699321\n","step: 20, loss = 0.0016696573002263904\n","step: 30, loss = 0.0010348360519856215\n","step: 40, loss = 0.0010109799914062023\n","step: 50, loss = 0.0021115485578775406\n","step: 60, loss = 0.001719985855743289\n","Running for epoch 33...\n","step: 0, loss = 0.0011766906827688217\n","step: 10, loss = 0.0003607063554227352\n","step: 20, loss = 0.0016694464720785618\n","step: 30, loss = 0.0010244997683912516\n","step: 40, loss = 0.0010000595357269049\n","step: 50, loss = 0.0020928012672811747\n","step: 60, loss = 0.0017196720000356436\n","Running for epoch 34...\n","step: 0, loss = 0.001159290666691959\n","step: 10, loss = 0.000358470220817253\n","step: 20, loss = 0.0016698550898581743\n","step: 30, loss = 0.001015958609059453\n","step: 40, loss = 0.0009909786749631166\n","step: 50, loss = 0.00207680300809443\n","step: 60, loss = 0.0017198994755744934\n","Running for epoch 35...\n","step: 0, loss = 0.001144454232417047\n","step: 10, loss = 0.0003569772816263139\n","step: 20, loss = 0.0016706769820302725\n","step: 30, loss = 0.0010088725248351693\n","step: 40, loss = 0.0009833984076976776\n","step: 50, loss = 0.0020631086081266403\n","step: 60, loss = 0.001720491680316627\n","Running for epoch 36...\n","step: 0, loss = 0.0011317615862935781\n","step: 10, loss = 0.0003560318145900965\n","step: 20, loss = 0.0016717601101845503\n","step: 30, loss = 0.0010029675904661417\n","step: 40, loss = 0.0009770423639565706\n","step: 50, loss = 0.002051349263638258\n","step: 60, loss = 0.00172131834551692\n","Running for epoch 37...\n","step: 0, loss = 0.0011208700016140938\n","step: 10, loss = 0.0003554861177690327\n","step: 20, loss = 0.0016729943454265594\n","step: 30, loss = 0.0009980262257158756\n","step: 40, loss = 0.0009716909844428301\n","step: 50, loss = 0.002041221596300602\n","step: 60, loss = 0.0017222848255187273\n","Running for epoch 38...\n","step: 0, loss = 0.001111492863856256\n","step: 10, loss = 0.00035522860707715154\n","step: 20, loss = 0.0016743007581681013\n","step: 30, loss = 0.000993872876279056\n","step: 40, loss = 0.0009671663865447044\n","step: 50, loss = 0.002032472752034664\n","step: 60, loss = 0.001723322900943458\n","Running for epoch 39...\n","step: 0, loss = 0.0011033962946385145\n","step: 10, loss = 0.00035517552169039845\n","step: 20, loss = 0.001675623469054699\n","step: 30, loss = 0.0009903664467856288\n","step: 40, loss = 0.000963323749601841\n","step: 50, loss = 0.0020248927175998688\n","step: 60, loss = 0.001724384492263198\n","Running for epoch 40...\n","step: 0, loss = 0.0010963844833895564\n","step: 10, loss = 0.000355264637619257\n","step: 20, loss = 0.0016769247595220804\n","step: 30, loss = 0.0009873933158814907\n","step: 40, loss = 0.0009600474731996655\n","step: 50, loss = 0.0020183068700134754\n","step: 60, loss = 0.0017254361882805824\n","Running for epoch 41...\n","step: 0, loss = 0.0010902934009209275\n","step: 10, loss = 0.0003554497961886227\n","step: 20, loss = 0.0016781799495220184\n","step: 30, loss = 0.0009848608169704676\n","step: 40, loss = 0.0009572422131896019\n","step: 50, loss = 0.0020125675946474075\n","step: 60, loss = 0.0017264559864997864\n","Running for epoch 42...\n","step: 0, loss = 0.001084985677152872\n","step: 10, loss = 0.0003556970623321831\n","step: 20, loss = 0.0016793729737401009\n","step: 30, loss = 0.0009826947934925556\n","step: 40, loss = 0.0009548304951749742\n","step: 50, loss = 0.0020075521897524595\n","step: 60, loss = 0.0017274296842515469\n","Running for epoch 43...\n","step: 0, loss = 0.0010803495533764362\n","step: 10, loss = 0.00035598172689788043\n","step: 20, loss = 0.0016804949846118689\n","step: 30, loss = 0.0009808349423110485\n","step: 40, loss = 0.0009527491056360304\n","step: 50, loss = 0.00200315797701478\n","step: 60, loss = 0.0017283488996326923\n","Running for epoch 44...\n","step: 0, loss = 0.001076288754120469\n","step: 10, loss = 0.00035628594923764467\n","step: 20, loss = 0.001681541558355093\n","step: 30, loss = 0.0009792308555915952\n","step: 40, loss = 0.0009509462397545576\n","step: 50, loss = 0.0019992985762655735\n","step: 60, loss = 0.0017292089760303497\n","Running for epoch 45...\n","step: 0, loss = 0.0010727214394137263\n","step: 10, loss = 0.0003565970400813967\n","step: 20, loss = 0.0016825118800625205\n","step: 30, loss = 0.000977842602878809\n","step: 40, loss = 0.0009493788820691407\n","step: 50, loss = 0.0019959001801908016\n","step: 60, loss = 0.0017300087492913008\n","Running for epoch 46...\n","step: 0, loss = 0.0010695811361074448\n","step: 10, loss = 0.00035690609365701675\n","step: 20, loss = 0.0016834074631333351\n","step: 30, loss = 0.0009766366565600038\n","step: 40, loss = 0.0009480123408138752\n","step: 50, loss = 0.0019929008558392525\n","step: 60, loss = 0.0017307483358308673\n","Running for epoch 47...\n","step: 0, loss = 0.0010668106842786074\n","step: 10, loss = 0.00035720696905627847\n","step: 20, loss = 0.0016842306358739734\n","step: 30, loss = 0.000975585775449872\n","step: 40, loss = 0.0009468165808357298\n","step: 50, loss = 0.001990248914808035\n","step: 60, loss = 0.001731429947540164\n","Running for epoch 48...\n","step: 0, loss = 0.0010643592104315758\n","step: 10, loss = 0.0003574959409888834\n","step: 20, loss = 0.0016849853564053774\n","step: 30, loss = 0.0009746666764840484\n","step: 40, loss = 0.0009457673877477646\n","step: 50, loss = 0.0019878980237990618\n","step: 60, loss = 0.0017320560291409492\n","Running for epoch 49...\n","step: 0, loss = 0.0010621885303407907\n","step: 10, loss = 0.00035777006996795535\n","step: 20, loss = 0.0016856752336025238\n","step: 30, loss = 0.0009738607914187014\n","step: 40, loss = 0.0009448440978303552\n","step: 50, loss = 0.0019858120940625668\n","step: 60, loss = 0.0017326296074315906\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y2naoTex3DFi","executionInfo":{"status":"ok","timestamp":1638183419284,"user_tz":-60,"elapsed":9,"user":{"displayName":"Quentin RAPILLY","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13440189952482936627"}},"outputId":"a3c7c5fa-5a59-4984-ffbe-573a729e5d7b"},"source":["#test\n","runnning_mape = 0\n","for i_batch, batch in enumerate(dataloader_test):\n","        x, y = batch\n","        x = torch.unsqueeze(x, -1).float()\n","        y = y.float()\n","        x, y = x.to(device), y.to(device)\n","        y_pred = model.forward(x)\n","        error = torch.mean(torch.abs((y - y_pred) / y))\n","        runnning_mape += error\n","\n","mape = runnning_mape / len(dataloader_test)\n","print(\"\",mape)"],"execution_count":134,"outputs":[{"output_type":"stream","name":"stdout","text":[" tensor(0.0292, device='cuda:0', grad_fn=<DivBackward0>)\n"]}]}]}